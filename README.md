# FineTuningLLM
 This project fine-tunes a LLaMA-2-based language model using the peft and trl libraries for efficient training. The model is loaded with 4-bit quantization for optimized performance and memory usage. It is trained on a dataset of medical terms and further customized with LoRA for efficient parameter tuning. The model generates text based on prompt
